{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = pd.read_csv(\"/opt/ml/input/data/item.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.868331303288674"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.title.apply(lambda x: len(x)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 모든 title을 전처리합니다. -> tokenizer로 전처리한 text 중에 길이가 5이하인 애들이 있습니다.\n",
    "# 전처리 전의 title이지만 평균 길이가 23이기에, 전처리 이후의 text 길이가 5이하인 애들은 제대로 클러스터링된 아이템들이 아닙니다. -> 클러스터링할 때 이 부분 고려합니다.\n",
    "# 전처리 후에 전처리 결과를 item df에 merge합니다.\n",
    "# TODO 전처리한 title의 similarity를 계산합니다. -> 전처리 title을 기준으로 item df를 sort합니다 -> 게슈탈트 패턴 매칭 알고리즘을 사용해서 계산합니다. -> 이는 재사용을 위해 pickle 객체로 저장합니다.\n",
    "# TODO item df를 전처리한 title 기준으로 sort 후에, 마찬가지로 전처리한 title 기준으로 정렬된 pickle 객체를 불러와서 merge합니다.\n",
    "# TODO 추천에 활용하지 않을 category 목록들을 필터링합니다.\n",
    "# TODO 필터링 이후, similarity에 대한 BFS 알고리즘을 적용합니다 -> 이전에 구한 similarity가 전체 item을 기준으로 구해졌기 때문에, 예외 처리를 해줘야 합니다.\n",
    "# TODO BFS 알고리즘을 통해 찾아낸 clustering group을 만들어주고, \"ETC\"나 \"etc\"로 group된 item들은 -1로 변환합니다.\n",
    "# TODO house_interaction에 클러스터 결과를 merge해줍니다. 이때, 클러스터 그룹이 -1이거나 house_interaction에만 포함된 item들은 cluster_id.max() + 1을 더해줍니다.\n",
    "# TODO model을 학습해서 학습한 결과를 backend에 반영합니다.\n",
    "# TODO cluster-major-item list를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "item[\"original_title\"] = item.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.rename(columns={\"title\":\"preprocessed_title\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_lower(df:pd.DataFrame):\n",
    "    df.preprocessed_title = df.preprocessed_title.apply(lambda x:str.lower(x))\n",
    "    return df\n",
    "\n",
    "# TODO 전처리: [단종], [품목], (당일출고) 등등 -> \"\" -> 재정렬\n",
    "def get_del_words(df:pd.DataFrame):\n",
    "    del_words = set()\n",
    "    from tqdm import tqdm\n",
    "    for a in tqdm(df.preprocessed_title.unique()):\n",
    "        tmp = a.strip(\"\\t\\n \")\n",
    "        if tmp.startswith(\"[\") and \"]\" in tmp:\n",
    "            tmp = tmp.split(\"]\")[0][1:]\n",
    "            del_words.add(\"[\" + tmp + \"]\")\n",
    "        elif tmp.startswith(\"(\") and \")\" in tmp:\n",
    "            tmp = tmp.split(\")\")[0][1:]\n",
    "            del_words.add(\"(\" + tmp + \")\")\n",
    "    return del_words\n",
    "\n",
    "def df_strip(df:pd.DataFrame, del_words:str = \"\\t\\n #&\"):\n",
    "    df.preprocessed_title = df.preprocessed_title.apply(lambda x:x.strip(del_words))\n",
    "    return df\n",
    "\n",
    "# TODO 전처리: 양끝에 \\t, \\n, \" \", ﻿ -> \"\"\n",
    "def df_strip2(df:pd.DataFrame):\n",
    "    df.preprocessed_title = df.preprocessed_title.apply(lambda x:x.strip(\"\\t\\n ﻿\"))\n",
    "    return df\n",
    "\n",
    "def df_del_word(df:pd.DataFrame, del_words:list):\n",
    "    def f(x, word):\n",
    "        if x.startswith(word):\n",
    "            return x[len(word):].strip(\" \")\n",
    "        else:\n",
    "            return x\n",
    "    from tqdm import tqdm\n",
    "    for word in tqdm(del_words):\n",
    "        df.preprocessed_title = df.preprocessed_title.apply(lambda x:f(x, word))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = df_lower(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146659/146659 [00:00<00:00, 811800.95it/s]\n",
      "100%|██████████| 2275/2275 [02:28<00:00, 15.32it/s]\n"
     ]
    }
   ],
   "source": [
    "del_list = get_del_words(item)\n",
    "item = df_del_word(item, del_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = df_strip(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = df_strip2(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.preprocessed_title = item.preprocessed_title.str.replace(pat=r'[^\\w]',repl=r' ',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.preprocessed_title.fillna(\"ETC\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "reg = re.compile(r'[a-zA-Z]')\n",
    "\n",
    "def tokenize(x:str):\n",
    "    okt = Okt() # 형태소 분석기 객체 생성\n",
    "    noun_list = []\n",
    "    x = x.split()\n",
    "    for s in x:\n",
    "        if reg.match(s):\n",
    "            noun_list.extend(text_to_word_sequence(s))\n",
    "        elif s.isdigit():\n",
    "            noun_list.append(s)\n",
    "        else:\n",
    "            noun_list.extend(okt.nouns(okt.normalize(s)))\n",
    "    return \" \".join(noun_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nintendo switch 90 포트 나이트 스페셜 세트'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Nintendo Switch 90 포트3나이트 스페셜 세트\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = item[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147780/147780 [04:26<00:00, 554.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "item.preprocessed_title = item.preprocessed_title.progress_apply(lambda x:tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.sort_values(\"item\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "now = str(round(time()))[5:]\n",
    "\n",
    "item.to_csv(f\"title_preprocessed_item_{now}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
